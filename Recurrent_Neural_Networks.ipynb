{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_graph(seed = 42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic RNN in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create an RNN composed of a layer of five recurrent neurons, using the tanh activation function. We will assume that the RNN runs over only two time steps, taking input vectors of size 3 at each time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\manog\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 3\n",
    "n_neurons = 5\n",
    "\n",
    "X0 = tf.placeholder(tf.float32, [None, n_inputs])\n",
    "X1 = tf.placeholder(tf.float32, [None, n_inputs])\n",
    "\n",
    "Wx = tf.Variable(tf.random_normal(shape = [n_inputs, n_neurons], dtype = tf.float32))\n",
    "Wy = tf.Variable(tf.random_normal(shape = [n_neurons,n_neurons], dtype = tf.float32))\n",
    "b = tf.Variable(tf.zeros([1, n_neurons], dtype = tf.float32))\n",
    "\n",
    "Y0 = tf.tanh(tf.matmul(X0, Wx) + b)\n",
    "Y1 = tf.tanh(tf.matmul(Y0, Wy) + tf.matmul(X1, Wx) + b)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mini-batch:        Instance 0,     1,         2,         3   \n",
    "X0_batch = np.array([[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 0, 1]])     #t = 0\n",
    "X1_batch = np.array([[9, 8, 7], [0, 0, 0], [6, 5, 4], [3, 2, 1]])     #t = 1\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    Y0_val, Y1_val = sess.run([Y0, Y1], feed_dict = {X0: X0_batch, X1: X1_batch})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This mini-batch contains four instances, each with an input sequence composed of exactly two inputs. At the end, Y0_val and Y1_val contain the outputs of the network at both time steps for all neurons and all instances in the mini-batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.0664006   0.9625767   0.68105793  0.7091854  -0.898216  ]\n",
      " [ 0.9977755  -0.719789   -0.9965761   0.9673924  -0.9998972 ]\n",
      " [ 0.99999774 -0.99898803 -0.9999989   0.9967762  -0.9999999 ]\n",
      " [ 1.         -1.         -1.         -0.99818915  0.9995087 ]]\n"
     ]
    }
   ],
   "source": [
    "print(Y0_val)    #output at t = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.         -1.         -1.          0.4020025  -0.9999998 ]\n",
      " [-0.12210419  0.62805265  0.9671843  -0.9937122  -0.2583937 ]\n",
      " [ 0.9999983  -0.9999994  -0.9999975  -0.85943305 -0.9999881 ]\n",
      " [ 0.99928284 -0.99999815 -0.9999058   0.9857963  -0.92205757]]\n"
     ]
    }
   ],
   "source": [
    "print(Y1_val)    #output at t = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using TensorFlow's RNN operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using static_rnn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The static_rnn() function creates an unrolled RNN network by chaining cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-8-4dbb7c2641fd>:7: BasicRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.SimpleRNNCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-8-4dbb7c2641fd>:8: static_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell, unroll=True)`, which is equivalent to this API\n"
     ]
    }
   ],
   "source": [
    "n_inputs = 3\n",
    "n_neurons = 5\n",
    "\n",
    "X0 = tf.placeholder(tf.float32, [None, n_inputs])\n",
    "X1 = tf.placeholder(tf.float32, [None, n_inputs])\n",
    "\n",
    "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units = n_neurons)\n",
    "output_seqs, states = tf.contrib.rnn.static_rnn(basic_cell, [X0, X1], dtype = tf.float32)\n",
    "\n",
    "Y0, Y1 = output_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X0_batch = np.array([[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 0, 1]])\n",
    "X1_batch = np.array([[9, 8, 7], [0, 0, 0], [6, 5, 4], [3, 2, 1]])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    Y0_val, Y1_val = sess.run([Y0, Y1], feed_dict = {X0: X0_batch, X1: X1_batch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.43304   ,  0.84509647, -0.49599633,  0.70050293, -0.59927976],\n",
       "       [ 0.03501973,  0.9954992 , -0.99537265,  0.9963196 , -0.95661724],\n",
       "       [-0.3744288 ,  0.9998789 , -0.9999681 ,  0.9999614 , -0.9960836 ],\n",
       "       [-0.9999052 ,  0.10390692, -0.9956875 ,  0.7440089 , -0.61860865]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y0_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-9.6329939e-01,  9.9918967e-01, -9.9999893e-01,  9.9997318e-01,\n",
       "        -9.9773598e-01],\n",
       "       [-1.4756016e-01, -5.8684641e-01, -3.7418893e-01, -7.4381530e-01,\n",
       "        -3.5059705e-01],\n",
       "       [-8.8282663e-01,  9.1975665e-01, -9.9985534e-01,  9.8879361e-01,\n",
       "        -9.5658535e-01],\n",
       "       [-6.0985601e-01, -2.0184807e-01, -9.7303903e-01, -3.0905005e-05,\n",
       "        -1.1064763e-01]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y1_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Packing sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 2\n",
    "n_inputs = 3\n",
    "n_neurons = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "X_seqs = tf.unstack(tf.transpose(X, perm = [1, 0, 2]))\n",
    "\n",
    "basic_cell = tf.nn.rnn_cell.BasicRNNCell(num_units = n_neurons)\n",
    "output_seqs, states = tf.nn.static_rnn(basic_cell, X_seqs, dtype = tf.float32)\n",
    "outputs = tf.transpose(tf.stack(output_seqs), perm = [1, 0, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can run the network by feeding it a single tensor that contains all the mini-batch sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "                    #  t = 0   ,   t = 1\n",
    "X_batch = np.array([[[0, 1, 2], [9, 8, 7]], # instance 1\n",
    "                    [[3, 4, 5], [0, 0, 0]], # instance 2\n",
    "                    [[6, 7, 8], [6, 5, 4]], # instance 3\n",
    "                    [[9, 0, 1], [3, 2, 1]], # instance 4\n",
    "                    ])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    outputs_val = outputs.eval(feed_dict = {X: X_batch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-0.45652324 -0.68064123  0.40938237  0.63104504 -0.45732826]\n",
      "  [-0.9428799  -0.9998869   0.94055814  0.9999985  -0.9999997 ]]\n",
      "\n",
      " [[-0.8001535  -0.9921827   0.7817797   0.9971032  -0.9964609 ]\n",
      "  [-0.637116    0.11300927  0.5798437   0.4310559  -0.6371699 ]]\n",
      "\n",
      " [[-0.93605185 -0.9998379   0.9308867   0.9999815  -0.99998295]\n",
      "  [-0.9165386  -0.9945604   0.896054    0.99987197 -0.9999751 ]]\n",
      "\n",
      " [[ 0.9927369  -0.9981933  -0.55543643  0.9989031  -0.9953323 ]\n",
      "  [-0.02746338 -0.73191994  0.7827872   0.9525682  -0.9781773 ]]]\n"
     ]
    }
   ],
   "source": [
    "print(outputs_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using dynamic_rnn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dynamic_rnn() function uses a while_loop() operation to run over the cell the appropriate number of times, and you can set \"swap_memory = True\" if you want it to swap the GPU’s memory to the CPU’s memory during backpropagation to avoid OOM errors.\n",
    "\n",
    "Conveniently, it also accepts a single tensor for all inputs at every time step (shape of [None, n_steps, n_inputs]) and it outputs a single tensor for all outputs at every time step (shape of [None, n_steps, n_neurons]); there is no need to stack, unstack, or transpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 2\n",
    "n_inputs = 3\n",
    "n_neurons = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-19-863adceb64a7>:6: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "\n",
    "basic_cell = tf.nn.rnn_cell.BasicRNNCell(num_units = n_neurons)\n",
    "outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_batch = np.array([\n",
    "                    [[0, 1, 2], [9, 8, 7]], # instance 1\n",
    "                    [[3, 4, 5], [0, 0, 0]], # instance 2\n",
    "                    [[6, 7, 8], [6, 5, 4]], # instance 3\n",
    "                    [[9, 0, 1], [3, 2, 1]], # instance 4\n",
    "                    ])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    outputs_val = outputs.eval(feed_dict = {X: X_batch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-0.85115266  0.87358344  0.5802911   0.8954789  -0.0557505 ]\n",
      "  [-0.999996    0.99999577  0.9981815   1.          0.37679607]]\n",
      "\n",
      " [[-0.9983293   0.9992038   0.98071456  0.999985    0.25192663]\n",
      "  [-0.7081804  -0.0772338  -0.85227895  0.5845349  -0.78780943]]\n",
      "\n",
      " [[-0.9999827   0.99999535  0.9992863   1.          0.5159072 ]\n",
      "  [-0.9993956   0.9984095   0.83422637  0.99999976 -0.47325212]]\n",
      "\n",
      " [[ 0.87888587  0.07356028  0.97216916  0.9998546  -0.7351168 ]\n",
      "  [-0.9134514   0.3600957   0.7624866   0.99817705  0.80142   ]]]\n"
     ]
    }
   ],
   "source": [
    "print(outputs_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling Variable-Length Input Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 2\n",
    "n_inputs = 3\n",
    "n_neurons = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\manog\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py:626: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "basic_cell = tf.nn.rnn_cell.BasicRNNCell(num_units = n_neurons)\n",
    "\n",
    "seq_length = tf.placeholder(tf.int32, [None])\n",
    "outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype = tf.float32, sequence_length = seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "                   #  step 0  ,  step 1 \n",
    "X_batch = np.array([\n",
    "                    [[0, 1, 2], [9, 8, 7]], # instance 1\n",
    "                    [[3, 4, 5], [0, 0, 0]], # instance 2 (padded with zero vectors)\n",
    "                    [[6, 7, 8], [6, 5, 4]], # instance 3\n",
    "                    [[9, 0, 1], [3, 2, 1]], # instance 4\n",
    "                    ])\n",
    "seq_length_batch = np.array([2, 1, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    outputs_val, states_val = sess.run([outputs, states], feed_dict = {X: X_batch, seq_length: seq_length_batch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-0.9123188   0.16516446  0.5548655  -0.39159346  0.20846416]\n",
      "  [-1.          0.956726    0.99831694  0.99970174  0.96518576]]\n",
      "\n",
      " [[-0.9998612   0.6702289   0.9723653   0.6631046   0.74457586]\n",
      "  [ 0.          0.          0.          0.          0.        ]]\n",
      "\n",
      " [[-0.99999976  0.8967997   0.9986295   0.9647514   0.93662   ]\n",
      "  [-0.9999526   0.9681953   0.96002865  0.98706263  0.85459226]]\n",
      "\n",
      " [[-0.96435434  0.99501586 -0.36150697  0.9983378   0.999497  ]\n",
      "  [-0.9613586   0.9568762   0.7132288   0.97729224 -0.0958299 ]]]\n"
     ]
    }
   ],
   "source": [
    "print(outputs_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.          0.956726    0.99831694  0.99970174  0.96518576]\n",
      " [-0.9998612   0.6702289   0.9723653   0.6631046   0.74457586]\n",
      " [-0.9999526   0.9681953   0.96002865  0.98706263  0.85459226]\n",
      " [-0.9613586   0.9568762   0.7132288   0.97729224 -0.0958299 ]]\n"
     ]
    }
   ],
   "source": [
    "print(states_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a Sequence Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will train an RNN to classify MNIST images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-30-bf231f8f6bb4>:16: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_steps = 28\n",
    "n_inputs = 28\n",
    "n_neurons = 150\n",
    "n_outputs = 10\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "y = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "basic_cell = tf.nn.rnn_cell.BasicRNNCell(num_units = n_neurons)\n",
    "outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype = tf.float32)\n",
    "\n",
    "logits = tf.layers.dense(states, n_outputs)\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = logits)\n",
    "loss = tf.reduce_mean(xentropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "correct = tf.cast(tf.nn.in_top_k(logits, y, 1), tf.float32)\n",
    "accuracy = tf.reduce_mean(correct)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "X_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "X_test = X_test.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "y_train = y_train.astype(np.int32)\n",
    "y_test = y_test.astype(np.int32)\n",
    "X_valid, X_train = X_train[:5000], X_train[5000:]\n",
    "y_valid, y_train = y_train[:5000], y_train[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_batch(X, y, batch_size):\n",
    "    rnd_idx = np.random.permutation(len(X))\n",
    "    n_batches = len(X) // batch_size\n",
    "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
    "        X_batch, y_batch = X[batch_idx], y[batch_idx]\n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.reshape((-1, n_steps, n_inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Last batch accuracy: 0.9533333 Test accuracy: 0.9288\n",
      "1 Last batch accuracy: 0.96 Test accuracy: 0.9471\n",
      "2 Last batch accuracy: 0.96 Test accuracy: 0.9499\n",
      "3 Last batch accuracy: 0.96 Test accuracy: 0.9563\n",
      "4 Last batch accuracy: 0.98 Test accuracy: 0.9677\n",
      "5 Last batch accuracy: 0.93333334 Test accuracy: 0.9651\n",
      "6 Last batch accuracy: 0.98 Test accuracy: 0.9685\n",
      "7 Last batch accuracy: 0.96666664 Test accuracy: 0.9678\n",
      "8 Last batch accuracy: 0.97333336 Test accuracy: 0.9693\n",
      "9 Last batch accuracy: 0.99333334 Test accuracy: 0.9714\n",
      "10 Last batch accuracy: 0.98 Test accuracy: 0.9752\n",
      "11 Last batch accuracy: 0.9866667 Test accuracy: 0.9743\n",
      "12 Last batch accuracy: 0.94666666 Test accuracy: 0.9716\n",
      "13 Last batch accuracy: 0.97333336 Test accuracy: 0.9658\n",
      "14 Last batch accuracy: 1.0 Test accuracy: 0.9772\n",
      "15 Last batch accuracy: 0.98 Test accuracy: 0.974\n",
      "16 Last batch accuracy: 0.99333334 Test accuracy: 0.9779\n",
      "17 Last batch accuracy: 0.9866667 Test accuracy: 0.9775\n",
      "18 Last batch accuracy: 0.9866667 Test accuracy: 0.9713\n",
      "19 Last batch accuracy: 0.98 Test accuracy: 0.9724\n",
      "20 Last batch accuracy: 0.9866667 Test accuracy: 0.9702\n",
      "21 Last batch accuracy: 0.98 Test accuracy: 0.9758\n",
      "22 Last batch accuracy: 0.98 Test accuracy: 0.9782\n",
      "23 Last batch accuracy: 0.99333334 Test accuracy: 0.9778\n",
      "24 Last batch accuracy: 0.9866667 Test accuracy: 0.9745\n",
      "25 Last batch accuracy: 0.9866667 Test accuracy: 0.9741\n",
      "26 Last batch accuracy: 0.9866667 Test accuracy: 0.9784\n",
      "27 Last batch accuracy: 1.0 Test accuracy: 0.9808\n",
      "28 Last batch accuracy: 0.99333334 Test accuracy: 0.9787\n",
      "29 Last batch accuracy: 0.98 Test accuracy: 0.9813\n",
      "30 Last batch accuracy: 1.0 Test accuracy: 0.9777\n",
      "31 Last batch accuracy: 0.9866667 Test accuracy: 0.9779\n",
      "32 Last batch accuracy: 0.9866667 Test accuracy: 0.977\n",
      "33 Last batch accuracy: 0.96666664 Test accuracy: 0.9771\n",
      "34 Last batch accuracy: 0.9866667 Test accuracy: 0.9792\n",
      "35 Last batch accuracy: 1.0 Test accuracy: 0.9794\n",
      "36 Last batch accuracy: 0.98 Test accuracy: 0.9739\n",
      "37 Last batch accuracy: 1.0 Test accuracy: 0.9773\n",
      "38 Last batch accuracy: 0.98 Test accuracy: 0.9761\n",
      "39 Last batch accuracy: 0.99333334 Test accuracy: 0.9745\n",
      "40 Last batch accuracy: 1.0 Test accuracy: 0.9785\n",
      "41 Last batch accuracy: 1.0 Test accuracy: 0.9808\n",
      "42 Last batch accuracy: 1.0 Test accuracy: 0.9782\n",
      "43 Last batch accuracy: 0.98 Test accuracy: 0.9734\n",
      "44 Last batch accuracy: 1.0 Test accuracy: 0.9803\n",
      "45 Last batch accuracy: 1.0 Test accuracy: 0.9776\n",
      "46 Last batch accuracy: 0.99333334 Test accuracy: 0.9785\n",
      "47 Last batch accuracy: 1.0 Test accuracy: 0.9785\n",
      "48 Last batch accuracy: 0.99333334 Test accuracy: 0.9804\n",
      "49 Last batch accuracy: 1.0 Test accuracy: 0.9801\n",
      "50 Last batch accuracy: 0.97333336 Test accuracy: 0.9628\n",
      "51 Last batch accuracy: 0.9866667 Test accuracy: 0.9753\n",
      "52 Last batch accuracy: 0.99333334 Test accuracy: 0.979\n",
      "53 Last batch accuracy: 0.99333334 Test accuracy: 0.9776\n",
      "54 Last batch accuracy: 1.0 Test accuracy: 0.9786\n",
      "55 Last batch accuracy: 1.0 Test accuracy: 0.9771\n",
      "56 Last batch accuracy: 1.0 Test accuracy: 0.9806\n",
      "57 Last batch accuracy: 0.9866667 Test accuracy: 0.9758\n",
      "58 Last batch accuracy: 1.0 Test accuracy: 0.9802\n",
      "59 Last batch accuracy: 0.99333334 Test accuracy: 0.9703\n",
      "60 Last batch accuracy: 0.97333336 Test accuracy: 0.9773\n",
      "61 Last batch accuracy: 0.99333334 Test accuracy: 0.9731\n",
      "62 Last batch accuracy: 0.99333334 Test accuracy: 0.9811\n",
      "63 Last batch accuracy: 0.99333334 Test accuracy: 0.9785\n",
      "64 Last batch accuracy: 0.9866667 Test accuracy: 0.9763\n",
      "65 Last batch accuracy: 0.9866667 Test accuracy: 0.9804\n",
      "66 Last batch accuracy: 0.99333334 Test accuracy: 0.9805\n",
      "67 Last batch accuracy: 1.0 Test accuracy: 0.9758\n",
      "68 Last batch accuracy: 0.97333336 Test accuracy: 0.9782\n",
      "69 Last batch accuracy: 1.0 Test accuracy: 0.9805\n",
      "70 Last batch accuracy: 0.99333334 Test accuracy: 0.9771\n",
      "71 Last batch accuracy: 0.99333334 Test accuracy: 0.975\n",
      "72 Last batch accuracy: 0.99333334 Test accuracy: 0.977\n",
      "73 Last batch accuracy: 0.99333334 Test accuracy: 0.9797\n",
      "74 Last batch accuracy: 0.98 Test accuracy: 0.9785\n",
      "75 Last batch accuracy: 0.9866667 Test accuracy: 0.9804\n",
      "76 Last batch accuracy: 0.96666664 Test accuracy: 0.9674\n",
      "77 Last batch accuracy: 1.0 Test accuracy: 0.9789\n",
      "78 Last batch accuracy: 1.0 Test accuracy: 0.9775\n",
      "79 Last batch accuracy: 1.0 Test accuracy: 0.9785\n",
      "80 Last batch accuracy: 0.9866667 Test accuracy: 0.9786\n",
      "81 Last batch accuracy: 1.0 Test accuracy: 0.9763\n",
      "82 Last batch accuracy: 0.99333334 Test accuracy: 0.9815\n",
      "83 Last batch accuracy: 0.9866667 Test accuracy: 0.9774\n",
      "84 Last batch accuracy: 0.99333334 Test accuracy: 0.9795\n",
      "85 Last batch accuracy: 0.9866667 Test accuracy: 0.979\n",
      "86 Last batch accuracy: 0.99333334 Test accuracy: 0.9803\n",
      "87 Last batch accuracy: 0.9866667 Test accuracy: 0.9816\n",
      "88 Last batch accuracy: 0.99333334 Test accuracy: 0.9773\n",
      "89 Last batch accuracy: 0.99333334 Test accuracy: 0.9797\n",
      "90 Last batch accuracy: 0.9866667 Test accuracy: 0.9756\n",
      "91 Last batch accuracy: 1.0 Test accuracy: 0.979\n",
      "92 Last batch accuracy: 0.99333334 Test accuracy: 0.9791\n",
      "93 Last batch accuracy: 1.0 Test accuracy: 0.9803\n",
      "94 Last batch accuracy: 1.0 Test accuracy: 0.983\n",
      "95 Last batch accuracy: 0.99333334 Test accuracy: 0.9795\n",
      "96 Last batch accuracy: 0.99333334 Test accuracy: 0.9762\n",
      "97 Last batch accuracy: 1.0 Test accuracy: 0.98\n",
      "98 Last batch accuracy: 0.99333334 Test accuracy: 0.9786\n",
      "99 Last batch accuracy: 0.96666664 Test accuracy: 0.9681\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "batch_size = 150\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            X_batch = X_batch.reshape((-1, n_steps, n_inputs))\n",
    "            sess.run(training_op, feed_dict = {X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict = {X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict = {X: X_test, y: y_test})\n",
    "        print(epoch, \"Last batch accuracy:\", acc_train, \"Test accuracy:\", acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Training to predict Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will create the RNN. It will contain 100 recurrent neurons and we will unroll it over 20 time steps since each training instance will be 20 inputs long. Each input will contain only one feature (the value at that time). The targets are also sequences of 20 inputs, each containing a single value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_min, t_max = 0, 30\n",
    "resolution = 0.1\n",
    "\n",
    "def time_series(t):\n",
    "    return t * np.sin(t) / 3 + 2 * np.sin(t*5)\n",
    "\n",
    "def next_batch(batch_size, n_steps):\n",
    "    t0 = np.random.rand(batch_size, 1) * (t_max - t_min - n_steps * resolution)\n",
    "    Ts = t0 + np.arange(0., n_steps + 1) * resolution\n",
    "    ys = time_series(Ts)\n",
    "    return ys[:, :-1].reshape(-1, n_steps, 1), ys[:, 1:].reshape(-1, n_steps, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_steps = 20\n",
    "n_inputs = 1\n",
    "n_neurons = 100\n",
    "n_outputs = 1\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "y = tf.placeholder(tf.float32, [None, n_steps, n_outputs])\n",
    "cell = tf.contrib.rnn.BasicRNNCell(num_units = n_neurons, activation = tf.nn.relu)\n",
    "outputs, states = tf.nn.dynamic_rnn(cell, X, dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using OutputProjectionWrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each time step we now have an output vector of size 100. But what we actually want is a single output value at each time step. The simplest solution is to wrap the cell in an OutputProjectionWrapper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A cell wrapper acts like a normal cell, proxying every method call to an underlying cell, but it also adds some functionality. The OutputProjectionWrapper adds a fully connected layer of linear neurons (i.e., without any activation function) on top of each output (but it does not affect the cell state)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_steps = 20\n",
    "n_inputs = 1\n",
    "n_neurons = 100\n",
    "n_outputs = 1\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "y = tf.placeholder(tf.float32, [None, n_steps, n_outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell = tf.contrib.rnn.OutputProjectionWrapper(tf.contrib.rnn.BasicRNNCell(num_units = n_neurons, activation = tf.nn.relu), \n",
    "                                              output_size = n_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs, states = tf.nn.dynamic_rnn(cell, X, dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "\n",
    "loss = tf.reduce_mean(tf.square(outputs - y))     # MSE, insted of cross-entropy\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \tMSE: 10.261381\n",
      "100 \tMSE: 0.38792896\n",
      "200 \tMSE: 0.10900874\n",
      "300 \tMSE: 0.061354414\n",
      "400 \tMSE: 0.059336416\n",
      "500 \tMSE: 0.058288667\n",
      "600 \tMSE: 0.052280974\n",
      "700 \tMSE: 0.047044784\n",
      "800 \tMSE: 0.049216457\n",
      "900 \tMSE: 0.0473833\n",
      "1000 \tMSE: 0.047798716\n",
      "1100 \tMSE: 0.047832422\n",
      "1200 \tMSE: 0.041717164\n",
      "1300 \tMSE: 0.046195257\n",
      "1400 \tMSE: 0.04128444\n"
     ]
    }
   ],
   "source": [
    "n_iterations = 1500\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for iteration in range(n_iterations):\n",
    "        X_batch, y_batch = next_batch(batch_size, n_steps)\n",
    "        sess.run(training_op, feed_dict = {X: X_batch, y: y_batch})\n",
    "        if iteration % 100 == 0:\n",
    "            mse = loss.eval(feed_dict = {X: X_batch, y: y_batch})\n",
    "            print(iteration, \"\\tMSE:\", mse)\n",
    "    \n",
    "    saver.save(sess, \"./my_time_series_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without using OutputProjectionWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_steps = 20\n",
    "n_inputs = 1\n",
    "n_neurons = 100\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "y = tf.placeholder(tf.float32, [None, n_steps, n_outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell = tf.nn.rnn_cell.BasicRNNCell(num_units = n_neurons, activation = tf.nn.relu)\n",
    "rnn_outputs, states = tf.nn.dynamic_rnn(cell, X, dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_outputs = 1\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we stack all the outputs using the reshape() operation, apply the fully connected linear layer (without using any activation function; this is just a projection), and finally unstack all the outputs, again using reshape()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_rnn_outputs = tf.reshape(rnn_outputs, [-1, n_neurons])\n",
    "stacked_outputs = tf.layers.dense(stacked_rnn_outputs, n_outputs)\n",
    "outputs = tf.reshape(stacked_outputs, [-1, n_steps, n_outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.square(outputs - y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \tMSE: 13.907029\n",
      "100 \tMSE: 0.5056698\n",
      "200 \tMSE: 0.19735886\n",
      "300 \tMSE: 0.101214476\n",
      "400 \tMSE: 0.06850145\n",
      "500 \tMSE: 0.06291986\n",
      "600 \tMSE: 0.055129297\n",
      "700 \tMSE: 0.049436502\n",
      "800 \tMSE: 0.050434686\n",
      "900 \tMSE: 0.0482007\n",
      "1000 \tMSE: 0.04809868\n",
      "1100 \tMSE: 0.04982501\n",
      "1200 \tMSE: 0.041912545\n",
      "1300 \tMSE: 0.049292978\n",
      "1400 \tMSE: 0.043140374\n"
     ]
    }
   ],
   "source": [
    "n_iterations = 1500\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for iteration in range(n_iterations):\n",
    "        X_batch, y_batch = next_batch(batch_size, n_steps)\n",
    "        sess.run(training_op, feed_dict = {X: X_batch, y: y_batch})\n",
    "        if iteration % 100 == 0:\n",
    "            mse = loss.eval(feed_dict = {X: X_batch, y: y_batch})\n",
    "            print(iteration, \"\\tMSE:\", mse)\n",
    "    \n",
    "    saver.save(sess, \"./my_time_series_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creative RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that we have a model that can predict the future, we can use it to generate some creative sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_time_series_model\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./my_time_series_model\")\n",
    "\n",
    "    sequence = [0.] * n_steps\n",
    "    for iteration in range(300):\n",
    "        X_batch = np.array(sequence[-n_steps:]).reshape(1, n_steps, 1)\n",
    "        y_pred = sess.run(outputs, feed_dict = {X: X_batch})\n",
    "        sequence.append(y_pred[0, -1, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacking multiple layers of cells gives a deep RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement a deep RNN in TensorFlow, we can create several cells and stack them into a MultiRNNCell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 2\n",
    "n_steps = 5\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neurons = 100\n",
    "n_layers = 3      #stack three identical cells \n",
    "\n",
    "layers = [tf.nn.rnn_cell.BasicRNNCell(num_units=n_neurons) for layer in range(n_layers)]\n",
    "multi_layer_cell = tf.nn.rnn_cell.MultiRNNCell(layers)\n",
    "outputs, states = tf.nn.dynamic_rnn(multi_layer_cell, X, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_batch = np.random.rand(2, n_steps, n_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    outputs_val, states_val = sess.run([outputs, states], feed_dict = {X: X_batch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 5, 100)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distributing a Deep RNN Across Multiple GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can efficiently distribute deep RNNs across multiple GPUs by pinning each layer to a different GPU. However, if we try to create each cell in a different device() block, it will not work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we have to create our own cell wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class DeviceCellWrapper(tf.nn.rnn_cell.RNNCell):\n",
    "  def __init__(self, device, cell):\n",
    "    self._cell = cell\n",
    "    self._device = device\n",
    "\n",
    "  @property\n",
    "  def state_size(self):\n",
    "    return self._cell.state_size\n",
    "\n",
    "  @property\n",
    "  def output_size(self):\n",
    "    return self._cell.output_size\n",
    "\n",
    "  def __call__(self, inputs, state, scope=None):\n",
    "    with tf.device(self._device):\n",
    "        return self._cell(inputs, state, scope)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This wrapper simply proxies every method call to another cell, except it wraps the __call__() function within a device block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 5\n",
    "n_steps = 20\n",
    "n_neurons = 100\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape = [None, n_steps, n_inputs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can distribute each layer on a different GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "devices = [\"/gpu:0\", \"/gpu:1\", \"/gpu:2\"]\n",
    "cells = [DeviceCellWrapper(dev,tf.contrib.rnn.BasicRNNCell(num_units = n_neurons)) for dev in devices]\n",
    "multi_layer_cell = tf.contrib.rnn.MultiRNNCell(cells)\n",
    "outputs, states = tf.nn.dynamic_rnn(multi_layer_cell, X, dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we build a very deep RNN, it may end up overfitting the training set. To prevent that, a common technique is to apply dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 1\n",
    "n_neurons = 100\n",
    "n_layers = 3\n",
    "n_steps = 20\n",
    "n_outputs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "y = tf.placeholder(tf.float32, [None, n_steps, n_outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\manog\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py:1259: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "keep_prob = tf.placeholder_with_default(1.0, shape = ())\n",
    "\n",
    "cells = [tf.nn.rnn_cell.BasicRNNCell(num_units = n_neurons) for layer in range(n_layers)]\n",
    "cells_drop = [tf.nn.rnn_cell.DropoutWrapper(cell, input_keep_prob = keep_prob) for cell in cells]\n",
    "multi_layer_cell = tf.nn.rnn_cell.MultiRNNCell(cells_drop)\n",
    "rnn_outputs, states = tf.nn.dynamic_rnn(multi_layer_cell, X, dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "stacked_rnn_outputs = tf.reshape(rnn_outputs, [-1, n_neurons])\n",
    "stacked_outputs = tf.layers.dense(stacked_rnn_outputs, n_outputs)\n",
    "outputs = tf.reshape(stacked_outputs, [-1, n_steps, n_outputs])\n",
    "\n",
    "loss = tf.reduce_mean(tf.square(outputs - y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Training MSE: 15.988591\n",
      "100 Training MSE: 4.8960195\n",
      "200 Training MSE: 3.974632\n",
      "300 Training MSE: 3.2563307\n",
      "400 Training MSE: 2.876809\n",
      "500 Training MSE: 3.024914\n",
      "600 Training MSE: 3.0242488\n",
      "700 Training MSE: 3.1380572\n",
      "800 Training MSE: 3.9774392\n",
      "900 Training MSE: 4.0770907\n",
      "1000 Training MSE: 3.221789\n",
      "1100 Training MSE: 3.440483\n",
      "1200 Training MSE: 2.7763686\n",
      "1300 Training MSE: 3.4578915\n",
      "1400 Training MSE: 4.674328\n"
     ]
    }
   ],
   "source": [
    "n_iterations = 1500\n",
    "batch_size = 50\n",
    "train_keep_prob = 0.5\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for iteration in range(n_iterations):\n",
    "        X_batch, y_batch = next_batch(batch_size, n_steps)\n",
    "        _, mse = sess.run([training_op, loss], feed_dict = {X: X_batch, y: y_batch, keep_prob: train_keep_prob})\n",
    "        if iteration % 100 == 0:\n",
    "            print(iteration, \"Training MSE:\", mse)\n",
    "    \n",
    "    saver.save(sess, \"./my_dropout_time_series_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM (Long Short-Term Memory) Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-97-211ce6545825>:3: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units = n_neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 28\n",
    "n_inputs = 28\n",
    "n_neurons = 150\n",
    "n_outputs = 10\n",
    "n_layers = 3\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "y = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "lstm_cells = [tf.nn.rnn_cell.BasicLSTMCell(num_units = n_neurons) for layer in range(n_layers)]\n",
    "multi_cell = tf.nn.rnn_cell.MultiRNNCell(lstm_cells)\n",
    "outputs, states = tf.nn.dynamic_rnn(multi_cell, X, dtype = tf.float32)\n",
    "top_layer_h_state = states[-1][1]\n",
    "logits = tf.layers.dense(top_layer_h_state, n_outputs, name = \"softmax\")\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = logits)\n",
    "loss = tf.reduce_mean(xentropy, name = \"loss\")\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "correct = tf.cast(tf.nn.in_top_k(logits, y, 1), tf.float32)\n",
    "accuracy = tf.reduce_mean(correct)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(LSTMStateTuple(c=<tf.Tensor 'rnn/while/Exit_3:0' shape=(?, 150) dtype=float32>, h=<tf.Tensor 'rnn/while/Exit_4:0' shape=(?, 150) dtype=float32>),\n",
       " LSTMStateTuple(c=<tf.Tensor 'rnn/while/Exit_5:0' shape=(?, 150) dtype=float32>, h=<tf.Tensor 'rnn/while/Exit_6:0' shape=(?, 150) dtype=float32>),\n",
       " LSTMStateTuple(c=<tf.Tensor 'rnn/while/Exit_7:0' shape=(?, 150) dtype=float32>, h=<tf.Tensor 'rnn/while/Exit_8:0' shape=(?, 150) dtype=float32>))"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'rnn/while/Exit_8:0' shape=(?, 150) dtype=float32>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_layer_h_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Last batch accuracy: 0.97333336 Test accuracy: 0.9561\n",
      "1 Last batch accuracy: 0.94666666 Test accuracy: 0.9702\n",
      "2 Last batch accuracy: 0.9533333 Test accuracy: 0.9672\n",
      "3 Last batch accuracy: 1.0 Test accuracy: 0.9797\n",
      "4 Last batch accuracy: 0.96666664 Test accuracy: 0.9849\n",
      "5 Last batch accuracy: 0.99333334 Test accuracy: 0.9855\n",
      "6 Last batch accuracy: 0.9866667 Test accuracy: 0.9859\n",
      "7 Last batch accuracy: 0.99333334 Test accuracy: 0.9868\n",
      "8 Last batch accuracy: 1.0 Test accuracy: 0.9868\n",
      "9 Last batch accuracy: 0.99333334 Test accuracy: 0.9879\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "batch_size = 150\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            X_batch = X_batch.reshape((-1, n_steps, n_inputs))\n",
    "            sess.run(training_op, feed_dict = {X: X_batch, y: y_batch})\n",
    "        acc_batch = accuracy.eval(feed_dict = {X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict = {X: X_test, y: y_test})\n",
    "        print(epoch, \"Last batch accuracy:\", acc_batch, \"Test accuracy:\", acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRU (Gated Recurrent Unit) Cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GRU cell is a simplified version of the LSTM cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-102-0561863e07fa>:1: GRUCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.GRUCell, and will be replaced by that in Tensorflow 2.0.\n"
     ]
    }
   ],
   "source": [
    "gru_cell = tf.contrib.rnn.GRUCell(num_units = n_neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
